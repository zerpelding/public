{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "Zoe Erpelding, CPSC 322, Fall 2023, Notebook for HW-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "# 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    " Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "# 2. Exploring k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8cfc4",
   "metadata": {},
   "source": [
    "*Note*: For the following, you must use functions we've already defined as appropriate\n",
    "\n",
    "*TODO: Complete the following steps (you should read through the following before getting started to make sure you have a sense for what is being asked):*\n",
    "1. Discretize the mpg values in the auto table using three equal-width bins\n",
    "2. Normalize all of the columns except for model and origin\n",
    "3. Create a train and test set using holdout with approximately half of the rows in the test set\n",
    "4. Run knn over the train and test set to predict mpg class labels, using majority voting, k=5, the numerical columns cylinders, weight, and acceleration, and no nominal attributes. \n",
    "5. Print the resulting confusion matrix\n",
    "6. Calculate and print the (average) accuracy across mpg labels, and the macro average f-measure. \n",
    "7. Run steps 1-6 3 times (i.e., put 1-6 in a for loop that iterates three times).\n",
    "8. Redo 7 (as another for loop) that uses weighted voting instead.\n",
    "9. Compare the performance differences, if any, between the results from 6 and 7.\n",
    "10. Pick two other bin sizes (either equal width or hand-crafted cut-points) for mpg values and redo 6 (as another for loop) for each. Compare the results. \n",
    "11. Pick a bin size and voting method from above, and redo 6 but with two different k values (i.e., add two more for loops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213aee01",
   "metadata": {},
   "source": [
    "7. Steps 1-6 in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e39692ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   90    0    0\n",
      "       2   53    0    0\n",
      "       3   10    0    0\n",
      " average accuracy is 0.7254901960784315\n",
      "average f-measure is 0.24691358024691357\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   77    3    0\n",
      "       2   57    6    0\n",
      "       3    9    1    0\n",
      " average accuracy is 0.6949891067538125\n",
      "average f-measure is 0.2849888404283637\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   78    0    0\n",
      "       2   67    0    0\n",
      "       3    8    0    0\n",
      " average accuracy is 0.6732026143790849\n",
      "average f-measure is 0.22510822510822512\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    width = (summary_stat(auto, 'mpg', max) - summary_stat(auto, 'mpg', min))/ 3\n",
    "    discretize(auto, 'mpg', [summary_stat(auto, 'mpg', min) + width, summary_stat(auto, 'mpg', min) + width + width])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bd0eb",
   "metadata": {},
   "source": [
    "8. Another for loop but with weighted vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "808a233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    1    8   73\n",
      "       2    0    2   58\n",
      "       3    0    2    9\n",
      " average accuracy is 0.3856209150326797\n",
      "average f-measure is 0.06628574637032308\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    1    0   81\n",
      "       2    1    0   60\n",
      "       3    0    0   10\n",
      " average accuracy is 0.38126361655773416\n",
      "average f-measure is 0.049344375431331966\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    3    0   75\n",
      "       2    0    0   63\n",
      "       3    0    0   12\n",
      " average accuracy is 0.3986928104575164\n",
      "average f-measure is 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    width = (summary_stat(auto, 'mpg', max) - summary_stat(auto, 'mpg', min))/ 3\n",
    "    discretize(auto, 'mpg', [summary_stat(auto, 'mpg', min) + width, summary_stat(auto, 'mpg', min) + width + width])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, weighted_vote, 5, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359d5d8",
   "metadata": {},
   "source": [
    "9. Compare the performance differences, if any, between the results from 6 and 7.\n",
    "\n",
    "The majority vote seems to have a better accuracy and f-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a49b9",
   "metadata": {},
   "source": [
    "10. Pick two other bin sizes (either equal width or hand-crafted cut-points) for mpg values and redo 6 (as another for loop) for each. Compare the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a1e9b",
   "metadata": {},
   "source": [
    "Choice 1 of bin size: [< 17, 17-24, < 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee7ba97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   25   26    0\n",
      "       2   22   31    0\n",
      "       3   12   37    0\n",
      " average accuracy is 0.5773420479302832\n",
      "average f-measure is 0.29210472067614923\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0    3   48\n",
      "       2    0    2   47\n",
      "       3    0    2   51\n",
      " average accuracy is 0.5642701525054467\n",
      "average f-measure is 0.19466379516630772\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0    8   39\n",
      "       2    0   16   40\n",
      "       3    0    7   43\n",
      " average accuracy is 0.5904139433551198\n",
      "average f-measure is 0.289272030651341\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    discretize(auto, 'mpg', [17.0, 24.0])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4261cf",
   "metadata": {},
   "source": [
    "Choice 2 of bin size: [< 14, 14-25, > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0f3a3f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0   18    0\n",
      "       2    0   83    0\n",
      "       3    0   52    0\n",
      " average accuracy is 0.6949891067538125\n",
      "average f-measure is 0.2344632768361582\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0   20    0\n",
      "       2    0   92    0\n",
      "       3    0   41    0\n",
      " average accuracy is 0.7342047930283225\n",
      "average f-measure is 0.25034013605442174\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0   13    0\n",
      "       2    0   98    1\n",
      "       3    0   41    0\n",
      " average accuracy is 0.7603485838779956\n",
      "average f-measure is 0.2602921646746348\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    discretize(auto, 'mpg', [14.0, 25.0])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf866092",
   "metadata": {},
   "source": [
    "Results: The first choice of bins seems to have a better f-measure, but the second choice seems to result in a better accuracy. This is because more things are binned into the second bin for the second choice of bins, which screws the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a1322",
   "metadata": {},
   "source": [
    "11. Pick a bin size and voting method from above, and redo 6 but with two different k values (i.e., add two more for loops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515c2b4",
   "metadata": {},
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "504a28e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   33   14    1\n",
      "       2   31   23    3\n",
      "       3   23   24    1\n",
      " average accuracy is 0.5816993464052288\n",
      "average f-measure is 0.30548508214002296\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   13    8   24\n",
      "       2   24    9   23\n",
      "       3   14   12   26\n",
      " average accuracy is 0.5424836601307189\n",
      "average f-measure is 0.29953267973856207\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    0    3   42\n",
      "       2    0    7   41\n",
      "       3    0    2   58\n",
      " average accuracy is 0.616557734204793\n",
      "average f-measure is 0.2701492537313433\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    discretize(auto, 'mpg', [17.0, 24.0])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, majority_vote, 3, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be858c5",
   "metadata": {},
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ecfb4e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    4   40    5\n",
      "       2    0   56    4\n",
      "       3    0   38    6\n",
      " average accuracy is 0.6209150326797386\n",
      "average f-measure is 0.31055093812125184\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    5    1   34\n",
      "       2    3    1   54\n",
      "       3    4    0   51\n",
      " average accuracy is 0.5816993464052288\n",
      "average f-measure is 0.2504714071724381\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1    1    7   32\n",
      "       2    0    7   56\n",
      "       3    0    6   44\n",
      " average accuracy is 0.5599128540305011\n",
      "average f-measure is 0.23365722337218076\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # step 0\n",
    "    auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    auto.load('auto-mpg.txt')\n",
    "\n",
    "    auto = remove_duplicates(auto)\n",
    "    auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "    \n",
    "    # step 1\n",
    "    discretize(auto, 'mpg', [17.0, 24.0])\n",
    "\n",
    "    # step 2\n",
    "    normalize(auto, 'cyls')\n",
    "    normalize(auto, 'disp')\n",
    "    normalize(auto, 'hp')\n",
    "    normalize(auto, 'weight')\n",
    "    normalize(auto, 'accl')\n",
    "    normalize(auto, 'year')\n",
    "\n",
    "    # step 3\n",
    "    total_rows = auto.row_count()\n",
    "    test_size = floor(total_rows / 2)\n",
    "    train, test = holdout(auto, test_size)\n",
    "\n",
    "    # step 4\n",
    "    matrix = knn_eval(train, test, majority_vote, 10, 'mpg', ['cyls', 'weight', 'accl'], [])\n",
    "    print(matrix)\n",
    "\n",
    "    # step 5\n",
    "    avg_acc = (accuracy(matrix, 1) + accuracy(matrix, 2) + accuracy(matrix, 3)) / 3\n",
    "    print(f' average accuracy is {avg_acc}')\n",
    "\n",
    "    # step 6\n",
    "    f = []\n",
    "    for i in range(3):\n",
    "        num = 0\n",
    "        den = 0\n",
    "        if i+1 in matrix.columns():\n",
    "            num = (2*recall(matrix, i+1)*precision(matrix, i+1))\n",
    "            den = (recall(matrix, i+1) + precision(matrix, i+1))\n",
    "        else:\n",
    "            fi = 0\n",
    "        if den != 0:\n",
    "            fi = num/den\n",
    "        else:\n",
    "            fi = 0\n",
    "        f.append(fi)\n",
    "\n",
    "    avg_f = (f[0]+f[1]+f[2])/3\n",
    "    print(f'average f-measure is {avg_f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "# 3. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "*TODO: Write down any issues and/or challenges that you had with the assignment. In addition, write down your observations concerning section 2 and brief explanations for the results you obtained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65b746",
   "metadata": {},
   "source": [
    "I had some confusion about the knn_eval function but I understood better after talking with Bowers. I am still having small syntax errors which make make things more difficult when trying to code. What I've observed is that the function is still only okay at predicting mpg. No matter the conditions, accuracy seems to stay relatively constant (around 0.5) and f-measure stays low (around 0.3). I also realized how important the choice of bins is, because have the distribution of the bins very skewed messes with your accuracy and f-measure. The results I have show that purposefully chosen bins are probably better than equal-width bins, majority vote is probably better than weighted vote, and a higher k is probably better than a lower k. However, none of these results show dramatic differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
