{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "Zoe Erpelding, CPSC 322, Fall 2023, Notebook for HW-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "# 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    " Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6208afb",
   "metadata": {},
   "source": [
    "# 2. Auto MPG Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "\n",
    "#TODO: Clean the auto data by:\n",
    "#  1. removing all duplicate rows\n",
    "#  2. removing all rows with missing values in any of the columns\n",
    "#  Note: these two steps should be carried out with functions we've already defined\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, ['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f7bad",
   "metadata": {},
   "source": [
    "## Step 1: k-NN versus Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0f898",
   "metadata": {},
   "source": [
    "*TODO: Complete the following steps using the functions you implemented for HW-6 (and prior):*\n",
    "1. Discretize the mpg value in the auto table using three equal-width bins\n",
    "2. Normalize the weight (wt) and displacement (disp) attributes\n",
    "3. Evaluate knn using stratified k-fold cross validation (i.e., your knn_stratified() function) to predict mpg labels using 10 folds, a knn k-value of 7, majority voting, and only the weight and displacement attributes (as numeric columns). Display the resulting confusion matrix.\n",
    "4. Compute accuracy, precision, recall, and the f-measure over the resulting confusion matrix and display each. \n",
    "5. Repeat step 3 using the same parameters but using naive-bayes instead of knn (i.e., your naive_bayes_stratified() function). Be sure to use weight and displacement as continuous attributes. Display the resulting confusion matrix.\n",
    "6. Compute accraccy, precision, recall, and the f-measure over the resulting confusion matrix and display each.\n",
    "7. Compare the results from 4,5 and 6,7. Write down your thoughts and observations concerning the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea16efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = (summary_stat(auto, 'mpg', max) - summary_stat(auto, 'mpg', min))/ 3\n",
    "discretize(auto, 'mpg', [summary_stat(auto, 'mpg', min) + width, summary_stat(auto, 'mpg', min) + width + width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70585ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(auto, 'weight')\n",
    "normalize(auto, 'disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848104d2",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "028af984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   64   98    0\n",
      "       2    0  122    1\n",
      "       3   17    5    0\n"
     ]
    }
   ],
   "source": [
    "matrix = knn_stratified(auto, 10, 'mpg', majority_vote, 7, ['weight', 'disp'])\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853f8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy is 0.7372421281216068\n",
      "The average precision is 0.44411522633744854\n",
      "The average recall is 0.46231054903141616\n",
      "The f-measure is 0.45303026392364903\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc1147",
   "metadata": {},
   "source": [
    "##### Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96da77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  142   19    1\n",
      "       2   14   77   32\n",
      "       3    0    4   18\n"
     ]
    }
   ],
   "source": [
    "matrix_nb = naive_bayes_stratified(auto, 10, 'mpg', ['weight', 'disp'])\n",
    "print(matrix_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ef326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy is 0.8479913137893593\n",
      "The average precision is 0.6777325289089995\n",
      "The average recall is 0.7735804294069877\n",
      "The f-measure is 0.7224914760560671\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24b46a",
   "metadata": {},
   "source": [
    "In this scenario, Naive Bays has better results for accuracy, precision, recall, and f-measure. With KNN, recall is higher than precision. In comparison, with Naive Bayes, precision is higher than recall.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e4dbb",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Auto MPG Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39f9ee",
   "metadata": {},
   "source": [
    "*TODO: Experiment with different numbers of folds, different attributes, different knn k-values, and so on, to find the parameters that work best for each approach (knn verus naive bayes) on the mpg data set. Explain what you tried and write down your observations. Note that you don't have to try all possibilities. Instead use your expert judgement to find combinations that seem to produce good results after trying a few possibilities. Be sure to give your experiments and their results below (as executable code), and provide commentary on what you tried, why, and your observations on what works well and why.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c18c9",
   "metadata": {},
   "source": [
    "##### Experimenting with number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e20b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of knn is 0.7589576547231269\n",
      "The average precision of knn is 0.5016669878866505\n",
      "The average recall of knn is 0.4895020667378391\n",
      "The f-measure of knn is 0.4955098754121142\n",
      "The average accuracy of naive bayes is 0.8479913137893593\n",
      "The average precision of naive bayes is 0.677497358571062\n",
      "The average recall of naive bays is 0.7729280154754411\n",
      "The f-measure of naive bayes is 0.7220732596386512\n"
     ]
    }
   ],
   "source": [
    "# many folds\n",
    "matrix = knn_stratified(auto, 300, 'mpg', majority_vote, 7, ['weight', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')\n",
    "\n",
    "matrix_nb = naive_bayes_stratified(auto, 300, 'mpg', ['weight', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94461195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of knn is 0.7328990228013029\n",
      "The average precision of knn is 0.4280156339644292\n",
      "The average recall of knn is 0.45884773662551437\n",
      "The f-measure of knn is 0.4428957410976873\n",
      "The average accuracy of naive bayes is 0.8523344191096633\n",
      "The average precision of naive bayes is 0.6841135062873377\n",
      "The average recall of naive bays is 0.778348069675983\n",
      "The f-measure of naive bayes is 0.7281947584944585\n"
     ]
    }
   ],
   "source": [
    "# few folds\n",
    "matrix = knn_stratified(auto, 3, 'mpg', majority_vote, 7, ['weight', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')\n",
    "\n",
    "matrix_nb = naive_bayes_stratified(auto, 3, 'mpg', ['weight', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5cc08c",
   "metadata": {},
   "source": [
    "I looked at values for folds ranging from 3 to 300. The very large values maximize the accuracy and f-measure of knn but the smaller values maximize the naive bayes accuracy and f-measure. Therefore, I chose the value of 19, since it raises the measures of both methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702df6b",
   "metadata": {},
   "source": [
    "##### Experimenting with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0d0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of knn is 0.8414766558089033\n",
      "The average precision of knn is 0.5076576193312347\n",
      "The average recall of knn is 0.5525946000200742\n",
      "The f-measure of knn is 0.5291738210614111\n",
      "The average accuracy of naive bayes is 0.8610206297502714\n",
      "The average precision of naive bayes is 0.5756940789198853\n",
      "The average recall of naive bays is 0.5789877090690099\n",
      "The f-measure of naive bayes is 0.5773361965965706\n"
     ]
    }
   ],
   "source": [
    "matrix = knn_stratified(auto, 19, 'mpg', majority_vote, 25, ['accl', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')\n",
    "\n",
    "matrix_nb = naive_bayes_stratified(auto, 19, 'mpg', ['accl', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c904d",
   "metadata": {},
   "source": [
    "Using acceleration and disp results in a higher accuracy but a lower f-measure for naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be80bb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of knn is 0.8154180238870792\n",
      "The average precision of knn is 0.49071157771945173\n",
      "The average recall of knn is 0.5344273813108501\n",
      "The f-measure of knn is 0.5116373758882194\n",
      "The average accuracy of naive bayes is 0.8501628664495113\n",
      "The average precision of naive bayes is 0.6795781637717121\n",
      "The average recall of naive bays is 0.775638042575712\n",
      "The f-measure of naive bayes is 0.7244376119863578\n"
     ]
    }
   ],
   "source": [
    "matrix = knn_stratified(auto, 19, 'mpg', majority_vote, 25, ['accl','weight','disp'])\n",
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')\n",
    "\n",
    "matrix_nb = naive_bayes_stratified(auto, 19, 'mpg', ['accl','weight', 'disp'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08039d82",
   "metadata": {},
   "source": [
    "Using acceleration, weight, and disp maximizes the accuracies and f-measures of knn and naive bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e33571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of knn is 0.8545059717698154\n",
      "The average precision of knn is 0.526336898395722\n",
      "The average recall of knn is 0.5584161397169528\n",
      "The f-measure of knn is 0.5419021817244336\n",
      "The average accuracy of naive bayes is 0.6916395222584147\n",
      "The average precision of naive bayes is 0.5751040431517519\n",
      "The average recall of naive bays is 0.4387643371383208\n",
      "The f-measure of naive bayes is 0.4977670655965271\n"
     ]
    }
   ],
   "source": [
    "matrix = knn_stratified(auto, 19, 'mpg', majority_vote, 25, ['weight', 'accl', 'disp', 'year'], ['name', 'origin'])\n",
    "avg_accuracy = (accuracy(matrix, 1)+accuracy(matrix,2)+accuracy(matrix,3))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix, 1) + precision(matrix, 2) + precision(matrix, 3))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix, 1) + recall(matrix,2) + recall(matrix,3))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')\n",
    "\n",
    "matrix_nb = naive_bayes_stratified(auto, 19, 'mpg', ['weight', 'accl', 'disp', 'year'], ['name', 'origin'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 1)+accuracy(matrix_nb,2)+accuracy(matrix_nb,3))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 1) + precision(matrix_nb, 2) + precision(matrix_nb, 3))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 1) + recall(matrix_nb,2) + recall(matrix_nb,3))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef343b4",
   "metadata": {},
   "source": [
    "Using all the attributes results in a higher knn accuracy but a lower naive bayes accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a4929",
   "metadata": {},
   "source": [
    "Of all the combinations I tried, I found that acceleration, weight, and disp maximized the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292a97c",
   "metadata": {},
   "source": [
    "For k nearest neighbors, from experimenting with values from 5 to 35, I found that 25 maximized the knn accuracy and f-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "# 2. Titanic Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8cfc4",
   "metadata": {},
   "source": [
    "Load the titanic data set below. The attributes are *class*, *age*, *gender*, and *survival*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2916b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the titanic data set below. Note no cleaning is needed.\n",
    "titanic = DataTable(['class', 'age', 'gender', 'survival'])\n",
    "titanic.load('titanic.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421ec64",
   "metadata": {},
   "source": [
    "*TODO: Using the titanic data set, predict survival using both knn and naive bayes via stratified k-fold cross validation using each of the other attributes as categorical features. For both, use 4 folds. For knn use a k-value of 7 and majority voting. As above, show the resulting confusion matrix for both along with accuraccy, precision, recall, and f measure results.*\n",
    "\n",
    "Note that depending on your implementations, it may take some time for Python to finish running your evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b02442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_knn = knn_stratified(titanic, 4, 'survival', majority_vote, 7, [], ['class', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_knn)\n",
    "avg_accuracy = (accuracy(matrix_knn, 'yes')+accuracy(matrix_knn,'no'))/2\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_knn, 'yes') + precision(matrix_knn, 'no'))/2\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix_knn, 'yes') + recall(matrix_knn,'no'))/2\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba15812",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\hw6_notebook.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zmerp/hw6-zerpelding/hw6_notebook.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m matrix_nb \u001b[39m=\u001b[39m naive_bayes_stratified(titanic, \u001b[39m4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msurvival\u001b[39m\u001b[39m'\u001b[39m, [], [\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_eval.py:143\u001b[0m, in \u001b[0;36mnaive_bayes_stratified\u001b[1;34m(table, k_folds, label_col, cont_cols, cat_cols)\u001b[0m\n\u001b[0;32m    141\u001b[0m         tables\u001b[39m.\u001b[39mappend(t)\n\u001b[0;32m    142\u001b[0m train \u001b[39m=\u001b[39m union_all(tables)\n\u001b[1;32m--> 143\u001b[0m matrix \u001b[39m=\u001b[39m naive_bayes_eval(train, test, label_col, cont_cols, cat_cols)\n\u001b[0;32m    144\u001b[0m r \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m final_matrix:\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_eval.py:103\u001b[0m, in \u001b[0;36mnaive_bayes_eval\u001b[1;34m(train, test, label_col, continuous_cols, categorical_cols)\u001b[0m\n\u001b[0;32m     99\u001b[0m labelsprob \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m test:\n\u001b[0;32m    102\u001b[0m     \u001b[39m# predicting the label\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     labelsprob \u001b[39m=\u001b[39m naive_bayes(train, instance, label_col, continuous_cols, categorical_cols)\n\u001b[0;32m    105\u001b[0m     \u001b[39m#compare the prediction to the ground truth\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m matrix: \n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_learn.py:37\u001b[0m, in \u001b[0;36mnaive_bayes\u001b[1;34m(table, instance, label_col, continuous_cols, categorical_cols)\u001b[0m\n\u001b[0;32m     34\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     35\u001b[0m prob \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 37\u001b[0m tables \u001b[39m=\u001b[39m partition(table, [label_col])\n\u001b[0;32m     38\u001b[0m probc \u001b[39m=\u001b[39m []\n\u001b[0;32m     39\u001b[0m probxcs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_util.py:443\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(table, columns)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    442\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m final:\n\u001b[1;32m--> 443\u001b[0m         \u001b[39mif\u001b[39;00m row\u001b[39m.\u001b[39mselect(columns) \u001b[39m==\u001b[39m t[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mselect(columns):\n\u001b[0;32m    444\u001b[0m             t\u001b[39m.\u001b[39mappend(row\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m    445\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_table.py:168\u001b[0m, in \u001b[0;36mDataRow.select\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(columns)):\n\u001b[0;32m    167\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mduplicate column names\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 168\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues(columns)\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m DataRow(columns, values)\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_table.py:147\u001b[0m, in \u001b[0;36mDataRow.values\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mset\u001b[39m(columns) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns()):\n\u001b[0;32m    146\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mduplicate column names\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m[column] \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m columns]\n",
      "File \u001b[1;32mc:\\Users\\zmerp\\hw6-zerpelding\\data_table.py:147\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mset\u001b[39m(columns) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns()):\n\u001b[0;32m    146\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mduplicate column names\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m[column] \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m columns]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matrix_nb = naive_bayes_stratified(titanic, 4, 'survival', [], ['class', 'age', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      yes    no\n",
      "--------  -----  ----\n",
      "yes        1364   126\n",
      "no          361   350\n",
      "The average accuracy of naive bayes is 0.7787369377555656\n",
      "The average precision of naive bayes is 0.7630093776641091\n",
      "The average recall of naive bays is 0.7038503289628937\n",
      "The f-measure of naive bayes is 0.7322368990632001\n"
     ]
    }
   ],
   "source": [
    "print(matrix_nb)\n",
    "avg_accuracy = (accuracy(matrix_nb, 'yes')+accuracy(matrix_nb,'no'))/2\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 'yes') + precision(matrix_nb, 'no'))/2\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 'yes') + recall(matrix_nb,'no') )/2\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c645af2",
   "metadata": {},
   "source": [
    "# 3. Student Stress Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68ce9c",
   "metadata": {},
   "source": [
    "Load the student stress data set below. The attributes are given below in column order, where the short name to use is given in parenthesis: \n",
    "1. sleep_quality (sleep)\n",
    "2. living_conditions (living)\n",
    "3. basic_needs (basics)\n",
    "4. academic_performance (academic)\n",
    "5. study_load (study)\n",
    "6. future_career_concerns (career)\n",
    "7. social_support (social)\n",
    "8. extracurricular_activities (extra)\n",
    "9. stress_level (stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be851a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the student stress data set\n",
    "stress = DataTable(['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra', 'stress'])\n",
    "stress.load('student-stress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a367e",
   "metadata": {},
   "source": [
    "## Step 1: Initial kNN and Naive Bayes Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71c6bf",
   "metadata": {},
   "source": [
    "*TODO: Similar to the titanic analysis above, use stratified k-fold cross validation to evaluate knn and naive bayes for predicting student stress level (the stress attribute) using the other table attributes as categorical values. For both evaluations use 10 folds, and for knn use a k-value of 7 and majority voting. Give your resulting confusion matrices as well as accuracy, precision, recall, and f-measure values.*\n",
    "\n",
    "Note that depending on your implementations, it may take some time for Python to finish running your evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59787268",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_knn = knn_stratified(stress, 10, 'stress', majority_vote, 7, [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dd543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    0    1    2\n",
      "--------  ---  ---  ---\n",
      "       0   38   12  323\n",
      "       1    3  355    0\n",
      "       2    0  304   65\n",
      "The average accuracy of knn is 0.610909090909091\n",
      "The average precision of knn is 0.5411387147733847\n",
      "The average recall of knn is 0.423216182950892\n",
      "The f-measure of knn is 0.47496759098500096\n"
     ]
    }
   ],
   "source": [
    "print(matrix_knn)\n",
    "avg_accuracy = (accuracy(matrix_knn, 0)+accuracy(matrix_knn,1)+accuracy(matrix_knn,2))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_knn, 0) + precision(matrix_knn, 1)+precision(matrix_knn, 2))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix_knn, 0) + recall(matrix_knn,1)+recall(matrix_knn,2))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07435671",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_nb = naive_bayes_stratified(stress, 10, 'stress', [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a00a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    0    1    2\n",
      "--------  ---  ---  ---\n",
      "       0  322   16   35\n",
      "       1   20  314   24\n",
      "       2   22   15  332\n",
      "The average accuracy of naive bayes is 0.9199999999999999\n",
      "The average precision of naive bayes is 0.8812883904955516\n",
      "The average recall of naive bays is 0.8800315822789683\n",
      "The f-measure of naive bayes is 0.8806595379829616\n"
     ]
    }
   ],
   "source": [
    "print(matrix_nb)\n",
    "avg_accuracy = (accuracy(matrix_nb, 0)+accuracy(matrix_nb,1)+accuracy(matrix_nb, 2))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 0) + precision(matrix_nb, 1)+precision(matrix_nb, 2))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 0) + recall(matrix_nb,1) + recall(matrix_nb, 2))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7247cb1",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Student Stress Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127d0d2",
   "metadata": {},
   "source": [
    "*TODO: Rerun your evaluations in Step 1 by experimenting with different knn k-values and folds (similar to the Auto MPG steps). Display, comment on, and analyze the results of your experiments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\zmerp\\AppData\\Local\\Temp\\ipykernel_23596\\3836812780.py\", line 1, in <module>\n",
      "    matrix_knn = knn_stratified(stress, 2, 'stress', majority_vote, 10, [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\hw6-zerpelding\\data_eval.py\", line 184, in knn_stratified\n",
      "    matrix = knn_eval(train, test, vote_fun, k, label_col, num_cols, nom_cols)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\hw6-zerpelding\\data_eval.py\", line 274, in knn_eval\n",
      "    majority = vote_fun(instances, scores, label_col)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\hw6-zerpelding\\data_learn.py\", line -1, in majority_vote\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\zmerp\\anaconda\\envs\\CPSC322\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "matrix_knn = knn_stratified(stress, 30, 'stress', majority_vote, 20, [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])\n",
    "avg_accuracy = (accuracy(matrix_knn, 0)+accuracy(matrix_knn,1)+accuracy(matrix_knn,2))/3\n",
    "print(f'The average accuracy of knn is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_knn, 0) + precision(matrix_knn, 1)+precision(matrix_knn, 2))/3\n",
    "print(f'The average precision of knn is {avg_precision}')\n",
    "avg_recall = (recall(matrix_knn, 0) + recall(matrix_knn,1)+recall(matrix_knn,2))/3\n",
    "print(f'The average recall of knn is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of knn is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c258ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of naive bayes is 0.9206060606060605\n",
      "The average precision of naive bayes is 0.882187275070097\n",
      "The average recall of naive bays is 0.8809626809754301\n",
      "The f-measure of naive bayes is 0.8815745527524422\n"
     ]
    }
   ],
   "source": [
    "matrix_nb = naive_bayes_stratified(stress, 30, 'stress', [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])\n",
    "avg_accuracy = (accuracy(matrix_nb, 0)+accuracy(matrix_nb,1)+accuracy(matrix_nb, 2))/3\n",
    "print(f'The average accuracy of naive bayes is {avg_accuracy}')\n",
    "avg_precision = (precision(matrix_nb, 0) + precision(matrix_nb, 1)+precision(matrix_nb, 2))/3\n",
    "print(f'The average precision of naive bayes is {avg_precision}')\n",
    "avg_recall = (recall(matrix_nb, 0) + recall(matrix_nb,1) + recall(matrix_nb, 2))/3\n",
    "print(f'The average recall of naive bays is {avg_recall}')\n",
    "avg_fmeasure = (2*avg_recall*avg_precision)/(avg_recall+avg_precision)\n",
    "print(f'The f-measure of naive bayes is {avg_fmeasure}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "# 4. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "*TODO: Write down any issues and/or challenges that you had with the assignment. In addition, write down your observations concerning section 2 and brief explanations for the results you obtained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481159c",
   "metadata": {},
   "source": [
    "I did not have any issues with the code itself but on the jupyter notebook, by knn stratified and naive bayes stratefied run very slowly on my computer (upwards of 7 minutes for the stress file). This made it hard to test which number of folds and k. \n",
    "\n",
    "I was surprised by how knn and naive bayes methods were not maximized by the same number of folds or attributes. I noticed that in general, more folds are better than less, but too many can be problematic for the f-measure. In general, my results showed that under most conditions, naive bayes is more affective than knn. \n",
    "\n",
    "For the sleep data, I found that the number of folds did not affect the results of naive bayes much. My computer was too slow to process many runs at the knn function, but based on my testing, I chose a large number of folds and k=20 to maximize the knn.\n",
    "\n",
    "For the auto data, I found that a k value of 35 and 19 folds maximized the results. I chose acceleration, weight, and disp as the attributes after experimenting with the combinations of options. I was surprised by how large a k value still maximizes the results for the auto data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
